\documentclass{classrep}
\usepackage[utf8]{inputenc}
\usepackage{color}

\studycycle{Informatyka, studia dzienne, I st.}
\coursesemester{VI}

\coursename{Komputerowe systemy rozpoznawania}
\courseyear{2018/2019}

\courseteacher{dr inż. Marcin Kacprowicz}
\coursegroup{poniedziałek, 14:10}

\author{
	\studentinfo{Justyna Hubert}{210200} \and
	\studentinfo{Karol Podlewski}{210294}
}

\title{Zadanie 1: Ekstrakcja cech, miary podobieństwa, klasyfikacja}
\svnurl{https://github.com/hubjust/KSR}

\begin{document}
	\maketitle
	
	
	\section{Cel}
	Celem zadania było stworzenie aplikacji do klasyfikacji tekstów metodą k-NN, korzystając z różnych sposób ekstrakcji wektorów cech oraz istniejących miar podobieństwa porównać kategorie do tych przypisanych przez aplikację.
	
	\section{Wprowadzenie}	
	
		Zagadnieniem, jakim zajmowaliśmy się w ramach projektu jest klasyfikacja statystyczna, która jest rodzajem algorytmu statystycznego przydzielającego elementy do klas, bazując na cechach tych elementów. W ramach przeprowadzanego eksperymentu zaimplementowaliśmy klasyfikator k-najbliższych sąsiadów. \newline
		Algorytm k najbliższych sąsiadów, nazywany także algorytmem k-nn, należy do grupy algorytmów leniwych, czyli takich, które nie tworzą wewnętrznej reprezentacji danych uczących, lecz szukają rozwiązania dopiero w momencie pojawienia się wzorca testującego. Przechowuje wszystkie wzorce uczące, względem których wyznacza odległość wzorca testowego [1]. Metoda k-nn wyznacza k sąsiadów, do których badany element ma najmniejszą odległość w danej metryce, a następnie wyznacza wynik w oparciu o najczęstszy element, wśród k najbliższych. W przypadku naszego projektu odległość definiujemy jako skalę podobieństwa tekstów. \newline
		W ramach zadania zostały użyte 2 metody ekstrakcji cech: \newline
			- Term frequency - metoda polegająca na zliczeniu częstości występowania danego słowa w dokumencie. Obliczana jest z poniższego wzoru:
			$$
			tf_{i,j}
			= \frac{n_{i,j}}{\sum_{k}n{k,j}}
			$$
			
			- Inverse document frequency - metoda polegająca na wyznaczeniu, czy dane słowo występuje powszechnie we wszystkich dokumentach. Jest to logarytmicznie skalowana odwrotna część dokumentów zawierających wybrane słowo (uzyskana poprzez podzielenie całkowitej liczby dokumentów przez liczbę dokumentów zawierających ten termin). Obliczana jest z poniższego wzoru: \newline
			$$
			idf_{i}
			= \log\frac{|D|}{|\{d : t_{i} \in d\}|}
			$$
		
		Do obliczenia odległości tekstów posłużyliśmy się 3 metrykami:
		- metryka Euklidesowa - w celu obliczenia odległości $$ d_{e}(x,y) $$ między dwoma punktami $$ x, y $$ należy obliczyć pierwiastek kwadratowy z sumy drugich potęg różnic wartości współrzędnych o tych samych indeksach, zgodnie ze wzorem:
			$$
			d_{e}(x,y)= \sqrt{ (y_{1} - x_{1})^2 + \cdots + (y_{n} - x_{n})^2 }
			$$
			
		- metryka uliczna (Manhattan, miejska) - w celu obliczenia odległości $$ d_{e}(x,y) $$ między dwoma punktami $$ x, y $$ należy obliczyć sumę wartości bezwględnych różnic współrzędnych punktów $$ x $$ oraz $$ y $$, zgodnie ze wzorem:
			$$
			d_{m}(x,y)= \sum_{k=1}^{n} | x_{k} - y_{k} |
			$$
			
		- metryka Czebyszewa - w celu obliczenia odległości $$ d_{e}(x,y) $$ między dwoma punktami $$ x, y $$ należy obliczyć maksymalną wartość bezwględnych różnic współrzędnych punktów $$ x $$ oraz $$ y $$, zgodnie ze wzorem:
			$$
			d_{ch}(x,y)= \max_{i} |x_{i} - y_{i}|
			$$

	\section{Opis implementacji}
	Program został stworzony w języku C\#. Graficzny interfejs użytkownika został stworzony przy  wykorzystaniu Windows Presentation Foundation. Logika aplikacji została odseparowana od GUI, w zgodzie ze wzorcem projektowym Model-view-viewmodel (MVVM), poprzez implementacje trzech projektów (Logic, ViewModel i GUI).
	
	\newline TUTAJ JAKIS DIAGRAM UML \newline
	
	\subsection{Logic}
		Klasy Chebyshev, Euclidean oraz Manhattan odpowiadają za prawidłowe obliczenia odległości tekstów. Dziedziczą one z klasy abstrakcyjnej Metric. \newline
		
		Klasa Article odwzorowuje artykuły wczytane do programu. Przechowuje informacje o dokumencie takie jak: tytuł, tekst, tagi, przypisane tagi, wektor cech, odległość. \newline
		
		Klasa FeatureExtraction implementuje dwie metody ekstrakcji cech - term frequency oraz inverse document frequency. \newline
		
		Klasa FileReader odpowiada za poprawne wczytywanie plików do programu - wyselekcjoowanie wybranych przez nas informacji (tytuł, ciało dokumentu, przypisaną etykietkę) i na ich podstawie stworzenie obiektu klasy Article. Z wczytanego tekstu usuwane są słowa, które występują w podanej przez nas stop liście. Ten zabieg ma za zadanie wykluczyć terminy, które nie wnoszą kluczowych, dla nas, informacji. Następnie, ciało dokumentu zostaje poddane stemizacji, czyli usunięciu ze słowa końcówki fleksyjnej pozostawiając tylko rdzeń wyrazu. \newline
		
		Klasa KnnAlgorithm odpowiada za implementację algorytmu k-najbliższych sąsiadów. W tym miejscu wyliczane są wystąpienia słów w podanych dokumentach. \newline
		
		Klasa Sets odpowiedzialna jest za odpowiedni dobór danych testowych oraz treningowych. \newline
		
		Klasa TagCompatibilityChecker zawiera listę etukiet, które zostały podane w treści zadania (west-germany, usa, france, uk, canada, japan).
		
		
	\subsection{GUI}
		Projekt GUI (graphical user interface) implementuje przejrzysty oraz łatwy w obsłudze graficzny interfejs użytkownika. 
		
	\subsection{ViewModel}
		Projekt ViewModel ma za zadanie odseparować logikę programu od interfejsu graficznego. \newline
		
		Klasa MainViewModel przyjmuje dane wejściowe od użytkownika i reaguje na jego poczynania wywołując wybrane akcje z logiki programu oraz odpowiada za odświeżanie widoków w interfejsie graficznym.  
		
		
		
	{\color{blue}
		Należy tu zamieścić krótki i zwięzły opis zaprojektowanych klas oraz powiązań
		między nimi. Powinien się tu również znaleźć diagram UML  (diagram klas)
		prezentujący najistotniejsze elementy stworzonej aplikacji. Należy także
		podać, w jakim języku programowania została stworzona aplikacja. }
	
	\section{Materiały i metody}
	{\color{blue}
		W tym miejscu należy opisać, jak przeprowadzone zostały wszystkie badania,
		których wyniki i dyskusja zamieszczane są w dalszych sekcjach. Opis ten
		powinien być na tyle dokładny, aby osoba czytająca go potrafiła wszystkie
		przeprowadzone badania samodzielnie powtórzyć w celu zweryfikowania ich
		poprawności (a zatem m.in. należy zamieścić tu opis architektury sieci,
		wartości współczynników użytych w kolejnych eksperymentach, sposób
		inicjalizacji wag, metodę uczenia itp. oraz informacje o danych, na których
		prowadzone były badania). Przy opisie należy odwoływać się i stosować do
		opisanych w sekcji drugiej wzorów i oznaczeń, a także w jasny sposób opisać
		cel konkretnego testu. Najlepiej byłoby wyraźnie wyszczególnić (ponumerować)
		poszczególne eksperymenty tak, aby łatwo było się do nich odwoływać dalej.}
	
	\section{Wyniki}
	{\color{blue}
		W tej sekcji należy zaprezentować, dla każdego przeprowadzonego eksperymentu,
		kompletny zestaw wyników w postaci tabel, wykresów itp. Powinny być one tak
		ponazywane, aby było wiadomo, do czego się odnoszą. Wszystkie tabele i wykresy
		należy oczywiście opisać (opisać co jest na osiach, w kolumnach itd.) stosując
		się do przyjętych wcześniej oznaczeń. Nie należy tu komentować i interpretować
		wyników, gdyż miejsce na to jest w kolejnej sekcji. Tu również dobrze jest
		wprowadzić oznaczenia (tabel, wykresów) aby móc się do nich odwoływać
		poniżej.}
	
	\section{Dyskusja}
	{\color{blue}
		Sekcja ta powinna zawierać dokładną interpretację uzyskanych wyników
		eksperymentów wraz ze szczegółowymi wnioskami z nich płynącymi. Najcenniejsze
		są, rzecz jasna, wnioski o charakterze uniwersalnym, które mogą być istotne
		przy innych, podobnych zadaniach. Należy również omówić i wyjaśnić wszystkie
		napotakane problemy (jeśli takie były). Każdy wniosek powinien mieć poparcie
		we wcześniej przeprowadzonych eksperymentach (odwołania do konkretnych
		wyników). Jest to jedna z najważniejszych sekcji tego sprawozdania, gdyż
		prezentuje poziom zrozumienia badanego problemu.}
	\section{Wnioski}
	{\color{blue}W tej, przedostatniej, sekcji należy zamieścić podsumowanie
		najważniejszych wniosków z sekcji poprzedniej. Najlepiej jest je po prostu
		wypunktować. Znów, tak jak poprzednio, najistotniejsze są wnioski o
		charakterze uniwersalnym.}
	
	
	\begin{thebibliography}{}
		\bibitem{adam}
		Methods for the linguistic summarization of data - aplications of fuzzy sets and their extensions, Adam Niewiadomski, Akademicka Oficyna Wydawnicza EXIT, Warszawa 2008
		1. http://home.agh.edu.pl/~horzyk/lectures/miw/KNN.pdf
	\end{thebibliography}
\end{document}
